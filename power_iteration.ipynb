{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power Iteration (naive version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.86499773, 1.03084908, 0.79445385, 1.37315888, 1.0358633 ,\n",
       "        1.40337002, 1.79362058, 1.77484352, 0.21759045, 0.96175826],\n",
       "       [1.03084908, 1.07546615, 1.09368951, 0.95276913, 1.56635058,\n",
       "        1.44762651, 1.69619211, 0.88272785, 1.03172841, 0.76931197],\n",
       "       [0.79445385, 1.09368951, 1.67294312, 0.12186061, 0.54345833,\n",
       "        0.52911833, 0.24780724, 0.46647832, 0.86452259, 0.94532059],\n",
       "       [1.37315888, 0.95276913, 0.12186061, 1.98023059, 1.18915405,\n",
       "        1.79798394, 0.49838315, 0.39772726, 1.34696042, 1.73379659],\n",
       "       [1.0358633 , 1.56635058, 0.54345833, 1.18915405, 0.40116623,\n",
       "        0.88074901, 1.21412068, 1.26395549, 1.58728916, 1.6174794 ],\n",
       "       [1.40337002, 1.44762651, 0.52911833, 1.79798394, 0.88074901,\n",
       "        0.27077348, 1.25938715, 1.08052315, 0.43028142, 1.09485715],\n",
       "       [1.79362058, 1.69619211, 0.24780724, 0.49838315, 1.21412068,\n",
       "        1.25938715, 0.81573363, 0.83426988, 1.4914469 , 1.83891134],\n",
       "       [1.77484352, 0.88272785, 0.46647832, 0.39772726, 1.26395549,\n",
       "        1.08052315, 0.83426988, 0.87434838, 1.00280664, 0.58271887],\n",
       "       [0.21759045, 1.03172841, 0.86452259, 1.34696042, 1.58728916,\n",
       "        0.43028142, 1.4914469 , 1.00280664, 0.06150891, 1.49873085],\n",
       "       [0.96175826, 0.76931197, 0.94532059, 1.73379659, 1.6174794 ,\n",
       "        1.09485715, 1.83891134, 0.58271887, 1.49873085, 1.90326877]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 10\n",
    "A = np.random.rand(n, n)\n",
    "A = A + A.T # make A symmetric\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.35560133],\n",
       "       [0.32874916],\n",
       "       [0.19652408],\n",
       "       [0.34079338],\n",
       "       [0.32804731],\n",
       "       [0.30217417],\n",
       "       [0.3467013 ],\n",
       "       [0.26493572],\n",
       "       [0.27886507],\n",
       "       [0.37898169]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.random.rand(n, 1)\n",
    "prev_w = w\n",
    "step = 1\n",
    "while True:\n",
    "    m = A @ w\n",
    "    q = m / np.linalg.norm(m, ord=2)\n",
    "    w = q\n",
    "    \n",
    "    # check convergence\n",
    "    if np.allclose(prev_w, w):\n",
    "        print(f\"step: {step}\")\n",
    "        break\n",
    "        \n",
    "    prev_w = w\n",
    "    step += 1\n",
    "w "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.35560147, -0.32874921, -0.19652401, -0.34079324, -0.32804725,\n",
       "       -0.30217427, -0.34670113, -0.26493566, -0.27886525, -0.3789817 ])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for the first eigenvector\n",
    "U, S, V = np.linalg.svd(A)\n",
    "U[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orthogonal/Subspace/Simultaneous Iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.53635343, 1.0995942 , 0.78298528, 0.44229695, 0.71542666,\n",
       "        1.61019761, 0.56817193, 0.56420826, 1.07705562, 0.96010582],\n",
       "       [1.0995942 , 1.96838227, 0.22723578, 1.2852785 , 0.87481116,\n",
       "        1.32723255, 1.35853843, 0.5122554 , 0.23478604, 0.80805871],\n",
       "       [0.78298528, 0.22723578, 1.43758949, 1.614269  , 0.87205978,\n",
       "        1.06240182, 1.9143086 , 0.97744638, 0.59039953, 1.07525904],\n",
       "       [0.44229695, 1.2852785 , 1.614269  , 0.05859349, 0.74785264,\n",
       "        0.72295798, 1.21352639, 1.44842409, 1.37809892, 1.34577067],\n",
       "       [0.71542666, 0.87481116, 0.87205978, 0.74785264, 1.9066172 ,\n",
       "        1.57445473, 0.97870504, 0.4390443 , 1.34751752, 0.90618566],\n",
       "       [1.61019761, 1.32723255, 1.06240182, 0.72295798, 1.57445473,\n",
       "        1.17369225, 1.1744562 , 0.78269922, 0.77670078, 1.71834375],\n",
       "       [0.56817193, 1.35853843, 1.9143086 , 1.21352639, 0.97870504,\n",
       "        1.1744562 , 1.98984588, 1.64528258, 0.48491928, 0.37954049],\n",
       "       [0.56420826, 0.5122554 , 0.97744638, 1.44842409, 0.4390443 ,\n",
       "        0.78269922, 1.64528258, 0.62370161, 1.4381944 , 0.8889138 ],\n",
       "       [1.07705562, 0.23478604, 0.59039953, 1.37809892, 1.34751752,\n",
       "        0.77670078, 0.48491928, 1.4381944 , 0.44504789, 1.29747119],\n",
       "       [0.96010582, 0.80805871, 1.07525904, 1.34577067, 0.90618566,\n",
       "        1.71834375, 0.37954049, 0.8889138 , 1.29747119, 1.75328823]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 10\n",
    "k = 5 # Condition: k < n\n",
    "A = np.random.rand(n, n)\n",
    "A = A + A.T # make A symmetric\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1341\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.25735105, -0.25350843,  0.29268086, -0.09610868, -0.25215006],\n",
       "       [-0.29808414, -0.10066142, -0.30009771, -0.75056101, -0.07673818],\n",
       "       [-0.32972443,  0.38427924, -0.34658749,  0.24437909, -0.11696359],\n",
       "       [-0.3128189 ,  0.14621441,  0.70029291,  0.12469   ,  0.2776717 ],\n",
       "       [-0.31915077, -0.2937494 ,  0.10679183, -0.0146923 , -0.26531032],\n",
       "       [-0.36335942, -0.29057489, -0.01158965, -0.13931453,  0.35301818],\n",
       "       [-0.36274713,  0.59483957,  0.08232304, -0.2341962 ,  0.20676453],\n",
       "       [-0.28675955,  0.2638967 , -0.08541311,  0.21774622, -0.56093955],\n",
       "       [-0.27391111, -0.15805882, -0.43322372,  0.3580297 ,  0.50857986],\n",
       "       [-0.34001666, -0.37063584, -0.02126359,  0.31950212, -0.16935146]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = np.random.rand(n, k)\n",
    "prev_W = W\n",
    "step = 1\n",
    "while True:\n",
    "    M = A @ W\n",
    "    Q, R = np.linalg.qr(M)\n",
    "    W = Q\n",
    "    \n",
    "    # check convergence\n",
    "    if step != 1 and np.allclose(prev_W, W):\n",
    "        print(f\"step: {step}\")\n",
    "        break\n",
    "        \n",
    "    prev_W = W\n",
    "    step += 1\n",
    "W "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.25735105, -0.25350843,  0.29268083,  0.09610878,  0.25215006,\n",
       "         0.09187944,  0.33472863,  0.18419607, -0.38864174,  0.63684348],\n",
       "       [-0.29808414, -0.10066142, -0.30009797,  0.75056091,  0.07673818,\n",
       "         0.29567997, -0.03711958, -0.2208086 , -0.20403855, -0.25006245],\n",
       "       [-0.32972443,  0.38427924, -0.34658741, -0.24437921,  0.11696359,\n",
       "        -0.09587964, -0.13844269,  0.50285732, -0.48443403, -0.18489367],\n",
       "       [-0.3128189 ,  0.14621441,  0.70029295, -0.12468976, -0.2776717 ,\n",
       "         0.21216828, -0.21903388, -0.22040364, -0.27682714, -0.28196471],\n",
       "       [-0.31915077, -0.2937494 ,  0.10679182,  0.01469234,  0.26531032,\n",
       "        -0.78862428,  0.03600558, -0.2068629 , -0.00810884, -0.25251035]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U, S, V = np.linalg.svd(A)\n",
    "U[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orthogonal/Subspace/Simultaneous Iteration  (SVD version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick note, this version is trying to solve the following optimization problem: \n",
    "\\begin{equation}\n",
    "    \\mathrm{Tr}(W^{\\top}AW)\n",
    "\\end{equation}\n",
    ", where $A$ is a positive semi-definite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.55373903, 0.7967697 , 0.93933074, ..., 0.15810815, 0.62133368,\n",
       "        1.58148596],\n",
       "       [0.7967697 , 1.34386004, 0.7350697 , ..., 1.06869878, 0.95665333,\n",
       "        0.75774177],\n",
       "       [0.93933074, 0.7350697 , 0.35596531, ..., 1.04328974, 1.10822284,\n",
       "        0.93416954],\n",
       "       ...,\n",
       "       [0.15810815, 1.06869878, 1.04328974, ..., 0.20297079, 0.75046873,\n",
       "        1.17491218],\n",
       "       [0.62133368, 0.95665333, 1.10822284, ..., 0.75046873, 1.52146115,\n",
       "        1.15210532],\n",
       "       [1.58148596, 0.75774177, 0.93416954, ..., 1.17491218, 1.15210532,\n",
       "        0.26301273]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 50\n",
    "k = 5 # Condition: k < n\n",
    "A = np.random.rand(n, n)\n",
    "A = A + A.T # make A symmetric\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0\t loss: 47.00954792432825\n",
      "step: 1\t loss: 45.79466900257554\n",
      "step: 2\t loss: 45.53514087923723\n",
      "step: 3\t loss: 45.807936857805785\n",
      "step: 4\t loss: 46.35664578536742\n",
      "step: 5\t loss: 47.0298455516151\n",
      "step: 6\t loss: 47.73145214407699\n",
      "step: 7\t loss: 48.397692282606826\n",
      "step: 8\t loss: 48.988946561800475\n",
      "step: 9\t loss: 49.485990100589554\n",
      "step: 10\t loss: 49.88566228666839\n",
      "step: 11\t loss: 50.19551104979241\n",
      "step: 12\t loss: 50.42862513334469\n",
      "step: 13\t loss: 50.5996750007194\n",
      "step: 14\t loss: 50.72246394155917\n",
      "step: 15\t loss: 50.80876090808852\n",
      "step: 16\t loss: 50.86799249061633\n",
      "step: 17\t loss: 50.90740315919703\n",
      "step: 18\t loss: 50.93240974344658\n",
      "step: 19\t loss: 50.946991842352304\n",
      "step: 20\t loss: 50.95404295714195\n",
      "step: 21\t loss: 50.95565650624776\n",
      "step: 22\t loss: 50.953345813839086\n",
      "step: 23\t loss: 50.94820740757858\n",
      "step: 24\t loss: 50.941039550161356\n",
      "step: 25\t loss: 50.93242710054937\n",
      "step: 26\t loss: 50.9228018002401\n",
      "step: 27\t loss: 50.91248495693725\n",
      "step: 28\t loss: 50.901717663025394\n",
      "step: 29\t loss: 50.89068224260879\n",
      "step: 30\t loss: 50.87951754281738\n",
      "step: 31\t loss: 50.86832990489268\n",
      "step: 32\t loss: 50.8572010968065\n",
      "step: 33\t loss: 50.84619410076239\n",
      "step: 34\t loss: 50.835357378399934\n",
      "step: 35\t loss: 50.824728048732936\n",
      "step: 36\t loss: 50.81433428365755\n",
      "step: 37\t loss: 50.80419713553448\n",
      "step: 38\t loss: 50.7943319485697\n",
      "step: 39\t loss: 50.7847494619584\n",
      "step: 40\t loss: 50.775456682155905\n",
      "step: 41\t loss: 50.76645758014339\n",
      "step: 42\t loss: 50.757753654394264\n",
      "step: 43\t loss: 50.74934438948096\n",
      "step: 44\t loss: 50.741227632588156\n",
      "step: 45\t loss: 50.73339990467011\n",
      "step: 46\t loss: 50.72585665899768\n",
      "step: 47\t loss: 50.718592496909935\n",
      "step: 48\t loss: 50.71160134843541\n",
      "step: 49\t loss: 50.70487662383461\n",
      "step: 50\t loss: 50.69841134090325\n",
      "step: 51\t loss: 50.69219823194949\n",
      "step: 52\t loss: 50.68622983364142\n",
      "step: 53\t loss: 50.68049856235981\n",
      "step: 54\t loss: 50.674996777247124\n",
      "step: 55\t loss: 50.66971683278733\n",
      "step: 56\t loss: 50.664651122461684\n",
      "step: 57\t loss: 50.65979211478748\n",
      "step: 58\t loss: 50.65513238285446\n",
      "step: 59\t loss: 50.65066462830542\n",
      "step: 60\t loss: 50.64638170057526\n",
      "step: 61\t loss: 50.64227661208349\n",
      "step: 62\t loss: 50.63834254997919\n",
      "step: 63\t loss: 50.6345728849525\n",
      "step: 64\t loss: 50.630961177556244\n",
      "step: 65\t loss: 50.627501182418484\n",
      "step: 66\t loss: 50.6241868506749\n",
      "step: 67\t loss: 50.62101233090538\n",
      "step: 68\t loss: 50.61797196881666\n",
      "step: 69\t loss: 50.615060305883844\n",
      "step: 70\t loss: 50.612272077128665\n",
      "step: 71\t loss: 50.609602208192996\n",
      "step: 72\t loss: 50.607045811838255\n",
      "step: 73\t loss: 50.604598183987555\n",
      "step: 74\t loss: 50.602254799406595\n",
      "step: 75\t loss: 50.6000113071084\n",
      "step: 76\t loss: 50.59786352555297\n",
      "step: 77\t loss: 50.595807437702035\n",
      "step: 78\t loss: 50.593839185981075\n",
      "step: 79\t loss: 50.59195506719142\n",
      "step: 80\t loss: 50.5901515274093\n",
      "step: 81\t loss: 50.588425156902304\n",
      "step: 82\t loss: 50.58677268508874\n",
      "step: 83\t loss: 50.585190975560664\n",
      "step: 84\t loss: 50.58367702118779\n",
      "step: 85\t loss: 50.58222793931582\n",
      "step: 86\t loss: 50.58084096707131\n",
      "step: 87\t loss: 50.579513456779914\n",
      "step: 88\t loss: 50.57824287150605\n",
      "step: 89\t loss: 50.5770267807183\n",
      "step: 90\t loss: 50.57586285608315\n",
      "step: 91\t loss: 50.574748867390674\n",
      "step: 92\t loss: 50.57368267861082\n",
      "step: 93\t loss: 50.57266224408326\n",
      "step: 94\t loss: 50.57168560483697\n",
      "step: 95\t loss: 50.57075088504088\n",
      "step: 96\t loss: 50.5698562885823\n",
      "step: 97\t loss: 50.56900009577135\n",
      "step: 98\t loss: 50.568180660168316\n",
      "step: 99\t loss: 50.56739640553265\n",
      "step: 100\t loss: 50.566645822887935\n",
      "step: 101\t loss: 50.565927467702714\n",
      "step: 102\t loss: 50.5652399571811\n",
      "step: 103\t loss: 50.56458196766184\n",
      "step: 104\t loss: 50.56395223212105\n",
      "step: 105\t loss: 50.56334953777623\n",
      "step: 106\t loss: 50.5627727237874\n",
      "step: 107\t loss: 50.56222067905213\n",
      "step: 108\t loss: 50.561692340091554\n",
      "step: 109\t loss: 50.561186689023174\n",
      "step: 110\t loss: 50.5607027516184\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 3.93873399e-02,  1.15434852e-01,  2.55556385e-01,\n",
       "         1.25898970e-01, -2.42426825e-01],\n",
       "       [-4.84055467e-02,  6.99000162e-02,  1.81454037e-01,\n",
       "        -3.16888558e-02,  1.25442485e-01],\n",
       "       [ 1.38989730e-01, -1.80058432e-01,  1.68097544e-01,\n",
       "         2.00756430e-01, -2.94689089e-02],\n",
       "       [ 4.33150283e-02,  1.52592506e-01,  1.80183565e-01,\n",
       "        -5.20910529e-02,  8.82016720e-03],\n",
       "       [-2.03119774e-02,  4.90022942e-02,  1.07690506e-01,\n",
       "         9.47467886e-02,  7.24132407e-02],\n",
       "       [ 2.49038050e-01, -2.76081889e-01,  1.42238411e-01,\n",
       "         1.04999095e-01,  9.85500603e-02],\n",
       "       [ 5.00346990e-02,  1.44516934e-01,  1.16205612e-01,\n",
       "        -9.42899253e-03,  2.71713916e-02],\n",
       "       [-2.59186541e-02,  2.33194806e-01, -1.07980616e-01,\n",
       "         1.84468364e-01,  2.49062087e-02],\n",
       "       [-2.39961394e-01,  1.74020319e-02,  1.98939477e-01,\n",
       "         7.47183598e-02,  2.79212474e-01],\n",
       "       [ 1.70157760e-01,  1.65096626e-01,  3.80808330e-02,\n",
       "        -1.91767132e-01,  1.48463004e-01],\n",
       "       [-4.58122258e-02,  3.07138774e-02,  2.44119177e-01,\n",
       "        -2.41861137e-02,  1.70146088e-01],\n",
       "       [ 4.73927995e-02, -6.13891743e-02,  1.72619505e-02,\n",
       "         2.22966547e-01,  9.23733238e-02],\n",
       "       [ 2.31612672e-01, -3.41870901e-02, -1.67556189e-02,\n",
       "         2.30925338e-01, -1.08098795e-01],\n",
       "       [ 3.44162534e-02,  4.56261376e-02,  3.11449778e-01,\n",
       "        -2.38216559e-02, -1.00865532e-02],\n",
       "       [ 1.00263972e-01,  1.52327735e-01, -2.14556543e-01,\n",
       "         1.11857669e-01,  1.58694734e-01],\n",
       "       [ 1.80982168e-01,  1.49380747e-01, -1.71640201e-01,\n",
       "         1.91137627e-01, -5.23006195e-02],\n",
       "       [ 1.06096531e-02,  3.26317029e-01,  4.79242694e-02,\n",
       "        -1.65205996e-01,  1.67848534e-01],\n",
       "       [ 2.22804757e-01, -1.63817739e-01,  3.33675338e-02,\n",
       "        -2.46652330e-02,  2.41132931e-01],\n",
       "       [ 1.63354466e-01, -1.79447214e-01,  8.60350937e-02,\n",
       "         4.02667116e-03,  1.98790829e-01],\n",
       "       [ 8.46355334e-02,  1.26628694e-01,  7.83584220e-02,\n",
       "         8.13482527e-02, -7.76868360e-02],\n",
       "       [ 5.02744856e-02,  9.61435694e-02, -7.93160220e-02,\n",
       "         9.99428547e-02,  1.16901937e-01],\n",
       "       [ 3.06700658e-01, -1.47536343e-01, -2.13063557e-01,\n",
       "         7.44883644e-02,  2.33265769e-01],\n",
       "       [ 1.30177009e-01, -9.96840794e-02,  5.50389179e-02,\n",
       "         1.55667018e-01,  5.95125968e-02],\n",
       "       [-1.26009285e-01, -1.40638540e-01,  1.95992221e-02,\n",
       "         1.22882142e-01,  4.51062350e-01],\n",
       "       [-2.16978009e-02,  1.04623054e-02,  6.99903915e-02,\n",
       "         1.99456399e-01,  2.10953546e-02],\n",
       "       [-1.66899428e-02,  2.30420678e-01,  1.05695464e-01,\n",
       "        -8.40119977e-03,  7.85025569e-03],\n",
       "       [-1.24742551e-01,  1.03378521e-01,  3.41040123e-02,\n",
       "         2.25544259e-01,  5.28646326e-02],\n",
       "       [ 2.26789320e-01,  1.81155848e-01,  3.21002646e-04,\n",
       "        -1.41870659e-01,  4.52508299e-02],\n",
       "       [ 2.48628222e-02,  2.04535835e-01, -1.44113157e-01,\n",
       "         2.81629837e-01, -2.55035541e-02],\n",
       "       [ 1.04408575e-01,  1.50785339e-01,  1.20237383e-01,\n",
       "        -1.84607092e-01,  1.17262966e-01],\n",
       "       [ 8.04414651e-02,  8.92311241e-02,  5.89823738e-02,\n",
       "         2.25266693e-01, -1.25712348e-01],\n",
       "       [-3.03704105e-01,  1.54531902e-01,  2.81736566e-01,\n",
       "         1.67906020e-01,  7.60152890e-02],\n",
       "       [ 1.11930011e-01,  6.46476248e-02, -6.88622461e-02,\n",
       "         8.73711473e-02,  9.78397363e-02],\n",
       "       [ 1.38938563e-02,  1.54226307e-01, -1.37774808e-01,\n",
       "        -1.10527726e-02,  2.82422512e-01],\n",
       "       [ 1.73765602e-01,  2.14115196e-01, -8.69070255e-02,\n",
       "        -8.79579277e-02,  1.16769283e-01],\n",
       "       [-7.76616285e-02,  1.38496412e-01, -6.98743679e-02,\n",
       "         2.17951937e-01,  1.18514798e-01],\n",
       "       [ 2.54023962e-01,  4.25549108e-02,  8.31299353e-02,\n",
       "        -7.25774990e-02, -1.68508794e-02],\n",
       "       [ 1.10553571e-01, -8.67206352e-02, -2.56637653e-03,\n",
       "         2.84657758e-01, -4.62770204e-02],\n",
       "       [ 1.64128518e-01, -7.71164526e-03,  1.89716641e-02,\n",
       "         2.36932136e-02,  1.23920313e-01],\n",
       "       [ 2.06421563e-01,  1.05675525e-01,  1.05441027e-01,\n",
       "        -3.18432516e-02, -4.69950298e-02],\n",
       "       [-3.65264267e-02,  1.30921923e-01,  1.14862437e-02,\n",
       "         1.20170981e-01,  1.05588295e-01],\n",
       "       [ 5.87354964e-02,  1.85272174e-01, -8.24778815e-02,\n",
       "        -1.59892482e-02,  1.70086369e-01],\n",
       "       [ 2.00153491e-01,  1.50827888e-01,  7.11891370e-02,\n",
       "         1.06658717e-02, -1.31229989e-01],\n",
       "       [ 9.50769732e-03, -3.25455340e-02,  1.49210848e-01,\n",
       "         4.98603185e-02,  1.53233403e-01],\n",
       "       [ 1.64523352e-01,  2.01426263e-02,  1.35864758e-01,\n",
       "         1.12262199e-01, -1.06526086e-01],\n",
       "       [ 4.57962687e-02, -2.11446845e-02,  2.09986077e-01,\n",
       "        -6.04862957e-02,  1.57897513e-01],\n",
       "       [-6.30134259e-02,  1.88840189e-01, -4.02006938e-02,\n",
       "         2.71414466e-01, -4.31279266e-02],\n",
       "       [ 7.27307091e-04, -9.55013917e-02,  2.27409234e-01,\n",
       "         1.83918722e-01, -3.72001218e-03],\n",
       "       [ 2.17949351e-01,  1.43003753e-01,  2.03134314e-01,\n",
       "        -7.56392047e-02, -1.61380269e-01],\n",
       "       [ 9.47527076e-02, -2.90498624e-02,  2.38787743e-01,\n",
       "        -1.36218670e-02,  2.79050846e-02]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = lambda W: np.trace(W.T @ A @ W)\n",
    "W = np.random.rand(n, k)\n",
    "prev_loss = 0\n",
    "curr_loss = 0\n",
    "step = 0\n",
    "while True:\n",
    "    M = A @ W\n",
    "    U, S, V = np.linalg.svd(M, full_matrices=False)\n",
    "    W = U @ V\n",
    "    curr_loss = loss(W)\n",
    "    print(f\"step: {step}\\t loss: {curr_loss}\")\n",
    "    # check convergence\n",
    "    if step != 0 and np.allclose(prev_loss, curr_loss):\n",
    "        break\n",
    "    prev_loss = curr_loss\n",
    "    step += 1\n",
    "W "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized Power Iteration Method (GPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 50\n",
    "k = 40 # k < n \n",
    "# A = np.random.randint(-100, 100, size=(n, n))\n",
    "A = np.random.rand(n, n)\n",
    "A = (A + A.T) / 2 # symmetric \n",
    "B = np.random.rand(n, k)\n",
    "# B = np.zeros(shape=(n, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1 - loss: 36.28119680418079\n",
      "step: 2 - loss: 33.27565553506631\n",
      "step: 3 - loss: 17.609741608765844\n",
      "step: 4 - loss: 30.65872062981184\n",
      "step: 5 - loss: 30.12626673734856\n",
      "step: 6 - loss: 30.064735805717255\n",
      "step: 7 - loss: 17.006237047125133\n",
      "step: 8 - loss: 16.736349652315837\n",
      "step: 9 - loss: 25.06356664885461\n",
      "step: 10 - loss: 13.089227239680742\n",
      "step: 11 - loss: 14.213087722323586\n",
      "step: 12 - loss: 10.498107113782659\n",
      "step: 13 - loss: 6.002299585488695\n",
      "step: 14 - loss: 5.889242084616445\n",
      "step: 15 - loss: 7.162755667189609\n",
      "step: 16 - loss: -2.235040539411363\n",
      "step: 17 - loss: -0.7782592110227078\n",
      "step: 18 - loss: 1.4442749658827467\n",
      "step: 19 - loss: 3.538187576075382\n",
      "step: 20 - loss: 3.7132141386122766\n",
      "step: 21 - loss: -3.2056611933338752\n",
      "step: 22 - loss: 6.859483067433509\n",
      "step: 23 - loss: -3.9974146722506694\n",
      "step: 24 - loss: 0.7422850503844565\n",
      "step: 25 - loss: 4.8000978149503\n",
      "step: 26 - loss: -9.194827786205535\n",
      "step: 27 - loss: -10.43338823457044\n",
      "step: 28 - loss: -5.68870490697361\n",
      "step: 29 - loss: -5.124889688014103\n",
      "step: 30 - loss: -1.9596435195155364\n",
      "step: 31 - loss: 0.04546571872673821\n",
      "step: 32 - loss: -7.00874922494617\n",
      "step: 33 - loss: 0.4284931344084715\n",
      "step: 34 - loss: 4.606515944169434\n",
      "step: 35 - loss: 2.7484610185624767\n",
      "step: 36 - loss: -6.068595866270973\n",
      "step: 37 - loss: -1.3089723706719534\n",
      "step: 38 - loss: -6.359843937287632\n",
      "step: 39 - loss: 3.04207900374566\n",
      "step: 40 - loss: -5.630617286579692\n",
      "step: 41 - loss: -9.924039154966717\n",
      "step: 42 - loss: -2.9344053184976806\n",
      "step: 43 - loss: -12.023431574973667\n",
      "step: 44 - loss: -10.007184595990546\n",
      "step: 45 - loss: -4.340784601977334\n",
      "step: 46 - loss: -7.441456131468641\n",
      "step: 47 - loss: -1.9721096078033407\n",
      "step: 48 - loss: 0.11111025412730768\n",
      "step: 49 - loss: -5.835883751991877\n",
      "step: 50 - loss: -3.185212447942758\n",
      "step: 51 - loss: -1.203397863633123\n",
      "step: 52 - loss: -5.023183422924314\n",
      "step: 53 - loss: 4.555456060333027\n",
      "step: 54 - loss: -2.4925018390417946\n",
      "step: 55 - loss: -11.343556856128\n",
      "step: 56 - loss: -5.3581493572917935\n",
      "step: 57 - loss: -4.855716192803151\n",
      "step: 58 - loss: -6.58288141240181\n",
      "step: 59 - loss: -2.2313668492250756\n",
      "step: 60 - loss: -9.62786623466944\n",
      "step: 61 - loss: -7.5178718935854185\n",
      "step: 62 - loss: -14.440947408288169\n",
      "step: 63 - loss: -11.30810776086673\n",
      "step: 64 - loss: -11.341236366767102\n",
      "step: 65 - loss: 0.6815356516844506\n",
      "step: 66 - loss: -15.316481025816865\n",
      "step: 67 - loss: -5.3926663789212\n",
      "step: 68 - loss: -4.16227493625407\n",
      "step: 69 - loss: -7.07149183136722\n",
      "step: 70 - loss: -7.74892765689906\n",
      "step: 71 - loss: -1.6724956068391101\n",
      "step: 72 - loss: -4.876499758559085\n",
      "step: 73 - loss: -6.675729933602161\n",
      "step: 74 - loss: 0.38404202691985034\n",
      "step: 75 - loss: -8.146185167185864\n",
      "step: 76 - loss: -4.9296471150641175\n",
      "step: 77 - loss: -5.274014243788753\n",
      "step: 78 - loss: -3.2431742740048586\n",
      "step: 79 - loss: -8.885627871925054\n",
      "step: 80 - loss: -12.191786074415834\n",
      "step: 81 - loss: -5.263139663399658\n",
      "step: 82 - loss: -11.59686977061093\n",
      "step: 83 - loss: -4.050394340029694\n",
      "step: 84 - loss: -14.109678042350279\n",
      "step: 85 - loss: -7.449019970591445\n",
      "step: 86 - loss: -7.174972906513882\n",
      "step: 87 - loss: -4.275334151738241\n",
      "step: 88 - loss: -2.8372535676149955\n",
      "step: 89 - loss: -7.241602156858442\n",
      "step: 90 - loss: -11.54251538378175\n",
      "step: 91 - loss: -3.1581675464360206\n",
      "step: 92 - loss: -6.515967701429985\n",
      "step: 93 - loss: -9.472729657185377\n",
      "step: 94 - loss: -6.274994646079741\n",
      "step: 95 - loss: -7.293340928941445\n",
      "step: 96 - loss: -5.894244063510328\n",
      "step: 97 - loss: -3.3756690664874025\n",
      "step: 98 - loss: -4.8037510652184245\n",
      "step: 99 - loss: -13.180875664568841\n",
      "step: 100 - loss: -8.098920429041856\n",
      "step: 101 - loss: -14.949507739213466\n",
      "step: 102 - loss: -8.088808820780132\n",
      "step: 103 - loss: -10.301636579663912\n",
      "step: 104 - loss: -11.47440184554667\n",
      "step: 105 - loss: -6.996353931855043\n",
      "step: 106 - loss: -4.316983575411374\n",
      "step: 107 - loss: -15.114374183684152\n",
      "step: 108 - loss: -12.81643268081464\n",
      "step: 109 - loss: -13.976846302150516\n",
      "step: 110 - loss: -6.323032048709926\n",
      "step: 111 - loss: -13.902934784913157\n",
      "step: 112 - loss: -9.964081013561628\n",
      "step: 113 - loss: -9.60517340799007\n",
      "step: 114 - loss: -9.125111066295961\n",
      "step: 115 - loss: -9.225656098517344\n",
      "step: 116 - loss: -12.322672981578885\n",
      "step: 117 - loss: -10.619245189222253\n",
      "step: 118 - loss: -10.099985615961442\n",
      "step: 119 - loss: -11.714182707396887\n",
      "step: 120 - loss: -13.68378283405739\n",
      "step: 121 - loss: -14.620623710409884\n",
      "step: 122 - loss: -13.254567076657883\n",
      "step: 123 - loss: -10.646510871889156\n",
      "step: 124 - loss: -9.291240905837828\n",
      "step: 125 - loss: -8.400648895822476\n",
      "step: 126 - loss: -13.491688718472528\n",
      "step: 127 - loss: -10.457192366928439\n",
      "step: 128 - loss: -13.112563707371022\n",
      "step: 129 - loss: -4.406480761983632\n",
      "step: 130 - loss: -6.1380888749091636\n",
      "step: 131 - loss: -4.674300644641682\n",
      "step: 132 - loss: -11.945454616142067\n",
      "step: 133 - loss: -12.139634315305972\n",
      "step: 134 - loss: -6.62081852839291\n",
      "step: 135 - loss: -5.844004083029681\n",
      "step: 136 - loss: -9.918683929680668\n",
      "step: 137 - loss: -8.629342653536762\n",
      "step: 138 - loss: -18.337438083355238\n",
      "step: 139 - loss: -12.748638078080326\n",
      "step: 140 - loss: -10.554615512599678\n",
      "step: 141 - loss: -11.90889014168941\n",
      "step: 142 - loss: -6.629818875660525\n",
      "step: 143 - loss: -10.024238940332923\n",
      "step: 144 - loss: -12.094272044128797\n",
      "step: 145 - loss: -7.947521512809734\n",
      "step: 146 - loss: -5.586140762418844\n",
      "step: 147 - loss: -19.76372770936557\n",
      "step: 148 - loss: -14.725567705023927\n",
      "step: 149 - loss: -3.5596583880143244\n",
      "step: 150 - loss: -9.034759231169232\n",
      "step: 151 - loss: -5.479267950826623\n",
      "step: 152 - loss: -14.018796860142142\n",
      "step: 153 - loss: -12.304871084424148\n",
      "step: 154 - loss: -9.574242623749626\n",
      "step: 155 - loss: -10.95029747082741\n",
      "step: 156 - loss: -14.324122947405677\n",
      "step: 157 - loss: -9.21537093979471\n",
      "step: 158 - loss: -11.956247661059468\n",
      "step: 159 - loss: -13.0804010514628\n",
      "step: 160 - loss: -8.950204712443593\n",
      "step: 161 - loss: -11.010417635370906\n",
      "step: 162 - loss: -15.150008664291803\n",
      "step: 163 - loss: -8.232959893751332\n",
      "step: 164 - loss: -7.428064295598133\n",
      "step: 165 - loss: -8.80951958312407\n",
      "step: 166 - loss: -12.610315933459393\n",
      "step: 167 - loss: -13.484754375854157\n",
      "step: 168 - loss: -10.244809108104008\n",
      "step: 169 - loss: -13.189938035368677\n",
      "step: 170 - loss: -8.366752690383063\n",
      "step: 171 - loss: -8.229807619063857\n",
      "step: 172 - loss: -9.554104926134677\n",
      "step: 173 - loss: -10.19714352371782\n",
      "step: 174 - loss: -16.7970799386111\n",
      "step: 175 - loss: -1.6970305872954619\n",
      "step: 176 - loss: -11.390775524359741\n",
      "step: 177 - loss: -17.82325581915406\n",
      "step: 178 - loss: -9.284474344011308\n",
      "step: 179 - loss: -11.873102987354947\n",
      "step: 180 - loss: -10.498397826382117\n",
      "step: 181 - loss: -10.082586514863653\n",
      "step: 182 - loss: -8.43741516415712\n",
      "step: 183 - loss: -11.471339747803684\n",
      "step: 184 - loss: -7.8131409439054895\n",
      "step: 185 - loss: -12.129459017758396\n",
      "step: 186 - loss: -9.655024272294739\n",
      "step: 187 - loss: -15.94312976199588\n",
      "step: 188 - loss: -18.417089519565643\n",
      "step: 189 - loss: -10.077992710332284\n",
      "step: 190 - loss: -15.462990988147649\n",
      "step: 191 - loss: -11.585003903403694\n",
      "step: 192 - loss: -8.055816619984581\n",
      "step: 193 - loss: -11.330545802894031\n",
      "step: 194 - loss: -10.98015811288021\n",
      "step: 195 - loss: -3.49862193354779\n",
      "step: 196 - loss: -6.739171757111136\n",
      "step: 197 - loss: -11.0017577235833\n",
      "step: 198 - loss: -14.401207007342613\n",
      "step: 199 - loss: -19.848116717542915\n",
      "step: 200 - loss: -9.681780624860236\n",
      "step: 201 - loss: -13.505454366912845\n",
      "step: 202 - loss: -13.405864376947415\n",
      "step: 203 - loss: -8.74952450383185\n",
      "step: 204 - loss: -10.685790668348718\n",
      "step: 205 - loss: -18.103984429410104\n",
      "step: 206 - loss: -16.27304733650248\n",
      "step: 207 - loss: -20.589482396055935\n",
      "step: 208 - loss: -9.37616370389479\n",
      "step: 209 - loss: -13.915290608613969\n",
      "step: 210 - loss: -9.593736991169479\n",
      "step: 211 - loss: -13.990898971781604\n",
      "step: 212 - loss: -16.877593686923966\n",
      "step: 213 - loss: -18.466741506485942\n",
      "step: 214 - loss: -18.990330401063794\n",
      "step: 215 - loss: -10.43309775927111\n",
      "step: 216 - loss: -17.373995724024603\n",
      "step: 217 - loss: -10.920250103268824\n",
      "step: 218 - loss: -8.527146111454451\n",
      "step: 219 - loss: -13.495948123951672\n",
      "step: 220 - loss: -13.21513157854908\n",
      "step: 221 - loss: -12.875829526652458\n",
      "step: 222 - loss: -16.825838465815828\n",
      "step: 223 - loss: -13.26917833921588\n",
      "step: 224 - loss: -18.22641180350105\n",
      "step: 225 - loss: -15.360788031703489\n",
      "step: 226 - loss: -17.872512674462392\n",
      "step: 227 - loss: -12.9084981012801\n",
      "step: 228 - loss: -16.416831540593808\n",
      "step: 229 - loss: -11.864826200505961\n",
      "step: 230 - loss: -11.643614012857581\n",
      "step: 231 - loss: -10.882066190557023\n",
      "step: 232 - loss: -11.688779346635075\n",
      "step: 233 - loss: -17.273151442418243\n",
      "step: 234 - loss: -8.41462128909842\n",
      "step: 235 - loss: -9.16403187543977\n",
      "step: 236 - loss: -8.075226817038718\n",
      "step: 237 - loss: -16.112156066384408\n",
      "step: 238 - loss: -5.560025563133847\n",
      "step: 239 - loss: -16.63455655357466\n",
      "step: 240 - loss: -12.119755588101356\n",
      "step: 241 - loss: -11.500618728956152\n",
      "step: 242 - loss: -14.016462353012308\n",
      "step: 243 - loss: -12.33455875390753\n",
      "step: 244 - loss: -9.854102138929335\n",
      "step: 245 - loss: -4.350470731470456\n",
      "step: 246 - loss: -10.332720078313075\n",
      "step: 247 - loss: -4.233035943827391\n",
      "step: 248 - loss: -15.03038665471922\n",
      "step: 249 - loss: -7.775384709641859\n",
      "step: 250 - loss: -7.406113736205832\n",
      "step: 251 - loss: -13.231849723217053\n",
      "step: 252 - loss: -12.143865891110732\n",
      "step: 253 - loss: -14.75093793607981\n",
      "step: 254 - loss: -9.675147296829637\n",
      "step: 255 - loss: -19.49026236149278\n",
      "step: 256 - loss: -9.305715512684092\n",
      "step: 257 - loss: -12.195443915761603\n",
      "step: 258 - loss: -6.776511324733098\n",
      "step: 259 - loss: -13.030394073144004\n",
      "step: 260 - loss: -16.983283736829478\n",
      "step: 261 - loss: -20.821996771178753\n",
      "step: 262 - loss: -11.137795804395981\n",
      "step: 263 - loss: -7.74929105799552\n",
      "step: 264 - loss: -12.172735964206792\n",
      "step: 265 - loss: -7.185953448976528\n",
      "step: 266 - loss: -12.90864485685478\n",
      "step: 267 - loss: -12.749638305860026\n",
      "step: 268 - loss: -10.610348793915495\n",
      "step: 269 - loss: -17.210100466837055\n",
      "step: 270 - loss: -16.024984124201517\n",
      "step: 271 - loss: -0.6009801818149825\n",
      "step: 272 - loss: -11.145086096940496\n",
      "step: 273 - loss: -12.485935884048075\n",
      "step: 274 - loss: -7.577902887375509\n",
      "step: 275 - loss: -5.423527293096091\n",
      "step: 276 - loss: -12.971427451597542\n",
      "step: 277 - loss: -14.43191743707924\n",
      "step: 278 - loss: -10.738028633015409\n",
      "step: 279 - loss: -17.122855697807555\n",
      "step: 280 - loss: -13.492922314449226\n",
      "step: 281 - loss: -12.924968666184093\n",
      "step: 282 - loss: -11.245578286700919\n",
      "step: 283 - loss: -6.7606336398443245\n",
      "step: 284 - loss: -13.321433270200034\n",
      "step: 285 - loss: -3.6703294223829666\n",
      "step: 286 - loss: -12.303166181488205\n",
      "step: 287 - loss: -16.44699789549963\n",
      "step: 288 - loss: -10.315320708584423\n",
      "step: 289 - loss: -9.254900808473844\n",
      "step: 290 - loss: -12.197425515325367\n",
      "step: 291 - loss: -9.514917272873479\n",
      "step: 292 - loss: -13.317632691583494\n",
      "step: 293 - loss: -10.320134961884676\n",
      "step: 294 - loss: -22.00709772492501\n",
      "step: 295 - loss: -10.86203408437628\n",
      "step: 296 - loss: -16.60150052249463\n",
      "step: 297 - loss: -19.445223018435314\n",
      "step: 298 - loss: -6.702798017743114\n",
      "step: 299 - loss: -15.80529715347868\n",
      "step: 300 - loss: -11.43895009422184\n",
      "step: 301 - loss: -13.049086536502301\n",
      "step: 302 - loss: -7.723800153675415\n",
      "step: 303 - loss: -11.34815587595934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 304 - loss: -5.84864147429645\n",
      "step: 305 - loss: -10.080106953126956\n",
      "step: 306 - loss: -9.844132615777243\n",
      "step: 307 - loss: -14.285644954650294\n",
      "step: 308 - loss: -5.715734896744335\n",
      "step: 309 - loss: -10.152021356268591\n",
      "step: 310 - loss: -18.17232202081193\n",
      "step: 311 - loss: -11.980423049042656\n",
      "step: 312 - loss: -8.32591085339671\n",
      "step: 313 - loss: -13.41725594289482\n",
      "step: 314 - loss: -3.3384674992503793\n",
      "step: 315 - loss: -12.977615528967513\n",
      "step: 316 - loss: -9.697937498129717\n",
      "step: 317 - loss: -19.295108194317283\n",
      "step: 318 - loss: -8.247335748120665\n",
      "step: 319 - loss: -9.950882105784832\n",
      "step: 320 - loss: -12.278670446247173\n",
      "step: 321 - loss: -13.100994104793934\n",
      "step: 322 - loss: -14.074253990244031\n",
      "step: 323 - loss: -9.307113591885475\n",
      "step: 324 - loss: -9.682828928384364\n",
      "step: 325 - loss: -10.494852348878817\n",
      "step: 326 - loss: -11.222668549002162\n",
      "step: 327 - loss: -12.644216886075903\n",
      "step: 328 - loss: -15.622394939532398\n",
      "step: 329 - loss: -16.117631795665577\n",
      "step: 330 - loss: -18.651210020109527\n",
      "step: 331 - loss: -15.67577662423843\n",
      "step: 332 - loss: -11.652302470254957\n",
      "step: 333 - loss: -15.964488172696502\n",
      "step: 334 - loss: -16.233665548258735\n",
      "step: 335 - loss: -3.7698628214787018\n",
      "step: 336 - loss: -18.571993712894024\n",
      "step: 337 - loss: -11.845367686915107\n",
      "step: 338 - loss: -10.454751439041349\n",
      "step: 339 - loss: -8.555752226852007\n",
      "step: 340 - loss: -9.235951099816226\n",
      "step: 341 - loss: -12.86379113280536\n",
      "step: 342 - loss: -4.313184891306177\n",
      "step: 343 - loss: -9.459622189568854\n",
      "step: 344 - loss: -13.727144776464721\n",
      "step: 345 - loss: -13.514975542448173\n",
      "step: 346 - loss: -13.435885346663232\n",
      "step: 347 - loss: -13.927804316429642\n",
      "step: 348 - loss: -7.550336703796164\n",
      "step: 349 - loss: -9.521413279222493\n",
      "step: 350 - loss: -4.957097786508639\n",
      "step: 351 - loss: -11.564318451633696\n",
      "step: 352 - loss: -9.756525370338114\n",
      "step: 353 - loss: -11.347053480133994\n",
      "step: 354 - loss: -12.906465715163\n",
      "step: 355 - loss: -11.510640342337991\n",
      "step: 356 - loss: -9.476317378951542\n",
      "step: 357 - loss: -12.98413430952783\n",
      "step: 358 - loss: -19.460135068180325\n",
      "step: 359 - loss: -11.183928227879788\n",
      "step: 360 - loss: -11.518792276834537\n",
      "step: 361 - loss: -14.215901214326776\n",
      "step: 362 - loss: -15.882945121761294\n",
      "step: 363 - loss: -15.452920997739227\n",
      "step: 364 - loss: -5.123117183379876\n",
      "step: 365 - loss: -11.948173093955736\n",
      "step: 366 - loss: -10.294455142530825\n",
      "step: 367 - loss: -6.694411201965299\n",
      "step: 368 - loss: -12.747367499523131\n",
      "step: 369 - loss: -13.095918277338345\n",
      "step: 370 - loss: -11.567765043252795\n",
      "step: 371 - loss: -11.400824204138036\n",
      "step: 372 - loss: -12.41118501928737\n",
      "step: 373 - loss: -16.284520117046963\n",
      "step: 374 - loss: -12.959234645163987\n",
      "step: 375 - loss: -17.407750817301896\n",
      "step: 376 - loss: -8.822000315215288\n",
      "step: 377 - loss: -9.400257674334567\n",
      "step: 378 - loss: -15.947758251932177\n",
      "step: 379 - loss: -14.094558357367962\n",
      "step: 380 - loss: -10.996700468730772\n",
      "step: 381 - loss: -13.66661452513658\n",
      "step: 382 - loss: -12.364119678149809\n",
      "step: 383 - loss: -17.65052009172465\n",
      "step: 384 - loss: -11.992715310582994\n",
      "step: 385 - loss: -19.241156296045045\n",
      "step: 386 - loss: -16.335146153756103\n",
      "step: 387 - loss: -10.613569590470272\n",
      "step: 388 - loss: -8.122117068885602\n",
      "step: 389 - loss: -11.099797455272046\n",
      "step: 390 - loss: -14.259633841728819\n",
      "step: 391 - loss: -12.221284360908685\n",
      "step: 392 - loss: -13.365462050924085\n",
      "step: 393 - loss: -13.241820711022685\n",
      "step: 394 - loss: -17.931727765700817\n",
      "step: 395 - loss: -11.617809156242128\n",
      "step: 396 - loss: -3.5301183344162794\n",
      "step: 397 - loss: -12.157272851308669\n",
      "step: 398 - loss: -9.962302013587113\n",
      "step: 399 - loss: -9.004078344575404\n",
      "step: 400 - loss: -8.614149658003132\n",
      "step: 401 - loss: -3.357132205286822\n",
      "step: 402 - loss: -10.887795995712674\n",
      "step: 403 - loss: -13.390921387962834\n",
      "step: 404 - loss: -11.457579083763523\n",
      "step: 405 - loss: -6.896276563313196\n",
      "step: 406 - loss: -10.953440525731832\n",
      "step: 407 - loss: -5.918209880553504\n",
      "step: 408 - loss: -7.852791661474378\n",
      "step: 409 - loss: -8.732076170698988\n",
      "step: 410 - loss: -15.613578234089704\n",
      "step: 411 - loss: -15.843032801222492\n",
      "step: 412 - loss: -18.086558288543316\n",
      "step: 413 - loss: -8.157983148551988\n",
      "step: 414 - loss: -1.9591310289940083\n",
      "step: 415 - loss: -8.651241948325548\n",
      "step: 416 - loss: -16.62793903748793\n",
      "step: 417 - loss: -9.887394813751522\n",
      "step: 418 - loss: -10.259282332494267\n",
      "step: 419 - loss: -8.868916720528386\n",
      "step: 420 - loss: -8.905464129459382\n",
      "step: 421 - loss: -10.378422252312365\n",
      "step: 422 - loss: -12.342581781487965\n",
      "step: 423 - loss: -14.927155251329705\n",
      "step: 424 - loss: -16.413185677377076\n",
      "step: 425 - loss: -11.953143757300335\n",
      "step: 426 - loss: -20.402268392531887\n",
      "step: 427 - loss: -12.47123112424177\n",
      "step: 428 - loss: -13.860631158690868\n",
      "step: 429 - loss: -12.706761162585689\n",
      "step: 430 - loss: -11.379643789362488\n",
      "step: 431 - loss: -16.193310306924175\n",
      "step: 432 - loss: -16.88257277204203\n",
      "step: 433 - loss: -16.575280384105803\n",
      "step: 434 - loss: -11.629860042944603\n",
      "step: 435 - loss: -10.436994494582896\n",
      "step: 436 - loss: -9.088096731563605\n",
      "step: 437 - loss: -11.998993405305056\n",
      "step: 438 - loss: -11.546931237962465\n",
      "step: 439 - loss: -2.73385440033824\n",
      "step: 440 - loss: -10.282682441796975\n",
      "step: 441 - loss: -10.884816887864277\n",
      "step: 442 - loss: -17.985698866425253\n",
      "step: 443 - loss: -7.72971749996771\n",
      "step: 444 - loss: -20.25760822481101\n",
      "step: 445 - loss: -16.580666071166018\n",
      "step: 446 - loss: -10.11472211854666\n",
      "step: 447 - loss: -10.917468356563727\n",
      "step: 448 - loss: -14.010649913446942\n",
      "step: 449 - loss: -12.84755492361103\n",
      "step: 450 - loss: -7.677059511700156\n",
      "step: 451 - loss: -14.724484361849857\n",
      "step: 452 - loss: -14.286347423738599\n",
      "step: 453 - loss: -17.664173974439585\n",
      "step: 454 - loss: -22.871984500462435\n",
      "step: 455 - loss: -17.779035271735268\n",
      "step: 456 - loss: -8.801486290881217\n",
      "step: 457 - loss: -11.113613150785179\n",
      "step: 458 - loss: -14.687640925847512\n",
      "step: 459 - loss: -11.598497888834679\n",
      "step: 460 - loss: -18.875191149377386\n",
      "step: 461 - loss: -11.381051175052384\n",
      "step: 462 - loss: -12.945176625847738\n",
      "step: 463 - loss: -6.739928603323325\n",
      "step: 464 - loss: -11.011892825070522\n",
      "step: 465 - loss: -15.75865413124176\n",
      "step: 466 - loss: -15.189582142587374\n",
      "step: 467 - loss: -14.274533866722383\n",
      "step: 468 - loss: -14.32649980147962\n",
      "step: 469 - loss: -9.52480454410383\n",
      "step: 470 - loss: -11.692239326138722\n",
      "step: 471 - loss: -13.555727137953372\n",
      "step: 472 - loss: -18.441800550149416\n",
      "step: 473 - loss: -8.177244341290736\n",
      "step: 474 - loss: -10.405051658867155\n",
      "step: 475 - loss: -14.999717338436142\n",
      "step: 476 - loss: -9.223611932299736\n",
      "step: 477 - loss: -13.9339720189809\n",
      "step: 478 - loss: -14.414234543824488\n",
      "step: 479 - loss: -17.66721671865171\n",
      "step: 480 - loss: -12.000159956717255\n",
      "step: 481 - loss: -3.549585869945675\n",
      "step: 482 - loss: -6.342992045029506\n",
      "step: 483 - loss: -15.468998673470143\n",
      "step: 484 - loss: -14.6256364278004\n",
      "step: 485 - loss: -17.699095928347905\n",
      "step: 486 - loss: -9.004665763809852\n",
      "step: 487 - loss: -9.828936376675003\n",
      "step: 488 - loss: 1.0557216980799051\n",
      "step: 489 - loss: -7.740755229988638\n",
      "step: 490 - loss: -6.288889888514467\n",
      "step: 491 - loss: -12.54477069339824\n",
      "step: 492 - loss: -3.9710573216956586\n",
      "step: 493 - loss: -12.902120195738755\n",
      "step: 494 - loss: -10.357906978375942\n",
      "step: 495 - loss: -9.314757874010658\n",
      "step: 496 - loss: -21.35297488159958\n",
      "step: 497 - loss: -13.199444714956508\n",
      "step: 498 - loss: -11.966757332753405\n",
      "step: 499 - loss: -10.051174596780777\n",
      "step: 500 - loss: -8.891027888940467\n",
      "step: 501 - loss: -10.969483600926143\n",
      "step: 502 - loss: -9.220580924269814\n",
      "step: 503 - loss: -5.7268389521302066\n",
      "step: 504 - loss: -19.512492924177067\n",
      "step: 505 - loss: -9.82916665211073\n",
      "step: 506 - loss: -6.969620390587936\n",
      "step: 507 - loss: -16.23013065416114\n",
      "step: 508 - loss: -11.317597504758762\n",
      "step: 509 - loss: -13.372640119409173\n",
      "step: 510 - loss: -14.646810740236822\n",
      "step: 511 - loss: -13.99965439531852\n",
      "step: 512 - loss: -12.767447068287707\n",
      "step: 513 - loss: -17.467317316211076\n",
      "step: 514 - loss: -13.804562621459969\n",
      "step: 515 - loss: -9.781399335223556\n",
      "step: 516 - loss: -12.711636200984579\n",
      "step: 517 - loss: -9.916739444981332\n",
      "step: 518 - loss: -14.706940755999987\n",
      "step: 519 - loss: -13.57689137395072\n",
      "step: 520 - loss: -10.965185415954032\n",
      "step: 521 - loss: -11.051930300991312\n",
      "step: 522 - loss: -10.37838948907753\n",
      "step: 523 - loss: -9.903914598337153\n",
      "step: 524 - loss: -15.101512175656568\n",
      "step: 525 - loss: -12.707438903251957\n",
      "step: 526 - loss: -14.468698164988584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 527 - loss: -14.76201173418308\n",
      "step: 528 - loss: -12.71714614734686\n",
      "step: 529 - loss: -8.493283755800782\n",
      "step: 530 - loss: -9.625476140127352\n",
      "step: 531 - loss: -13.07744746647069\n",
      "step: 532 - loss: -5.1036361426290355\n",
      "step: 533 - loss: -2.1048005395752565\n",
      "step: 534 - loss: -5.968771926363183\n",
      "step: 535 - loss: -9.329001304541967\n",
      "step: 536 - loss: -14.328665935209962\n",
      "step: 537 - loss: -11.94855154186599\n",
      "step: 538 - loss: -12.752091786350803\n",
      "step: 539 - loss: -18.81932385635905\n",
      "step: 540 - loss: -9.867270682432643\n",
      "step: 541 - loss: -11.143687101927737\n",
      "step: 542 - loss: -8.40631921786727\n",
      "step: 543 - loss: -3.4220296843842437\n",
      "step: 544 - loss: -10.725670500237378\n",
      "step: 545 - loss: -11.295045025546845\n",
      "step: 546 - loss: -18.03087245780415\n",
      "step: 547 - loss: -13.536288812135567\n",
      "step: 548 - loss: -14.960224534875733\n",
      "step: 549 - loss: -12.644375757406715\n",
      "step: 550 - loss: -13.488655907169907\n",
      "step: 551 - loss: -16.114628930120688\n",
      "step: 552 - loss: -11.502015863961931\n",
      "step: 553 - loss: -15.13737034531653\n",
      "step: 554 - loss: -13.714676472876778\n",
      "step: 555 - loss: -15.251180407948155\n",
      "step: 556 - loss: -14.66366620684446\n",
      "step: 557 - loss: -15.458748446290683\n",
      "step: 558 - loss: -12.454216843612045\n",
      "step: 559 - loss: -8.066884009597754\n",
      "step: 560 - loss: -12.212498552959305\n",
      "step: 561 - loss: -9.116626539667843\n",
      "step: 562 - loss: -10.956376390155647\n",
      "step: 563 - loss: -13.827359723936132\n",
      "step: 564 - loss: -6.773289055240622\n",
      "step: 565 - loss: -11.128166346053906\n",
      "step: 566 - loss: -14.164395966811945\n",
      "step: 567 - loss: -11.072861996560544\n",
      "step: 568 - loss: -13.797099537937083\n",
      "step: 569 - loss: -6.1454066628551764\n",
      "step: 570 - loss: -10.734009137817749\n",
      "step: 571 - loss: -16.305750595667106\n",
      "step: 572 - loss: -11.603363261025235\n",
      "step: 573 - loss: -14.867316431572466\n",
      "step: 574 - loss: -21.053001687689253\n",
      "step: 575 - loss: -12.125341764560499\n",
      "step: 576 - loss: -9.883148231157826\n",
      "step: 577 - loss: -14.190398872142989\n",
      "step: 578 - loss: -13.79101334953226\n",
      "step: 579 - loss: -4.025400972331289\n",
      "step: 580 - loss: -11.791099167198\n",
      "step: 581 - loss: -5.219528557089336\n",
      "step: 582 - loss: -14.380101163890119\n",
      "step: 583 - loss: -13.559417758699226\n",
      "step: 584 - loss: -15.532049358223306\n",
      "step: 585 - loss: -11.049511327268174\n",
      "step: 586 - loss: -9.240689638241472\n",
      "step: 587 - loss: -11.675658500283001\n",
      "step: 588 - loss: -8.964091476684203\n",
      "step: 589 - loss: -10.638813248320897\n",
      "step: 590 - loss: -10.03705771207358\n",
      "step: 591 - loss: -17.641460704533287\n",
      "step: 592 - loss: -17.37384492054025\n",
      "step: 593 - loss: -19.627564192865485\n",
      "step: 594 - loss: -10.890715679327261\n",
      "step: 595 - loss: -5.9195804989382195\n",
      "step: 596 - loss: -15.798961035584776\n",
      "step: 597 - loss: -14.256381403333316\n",
      "step: 598 - loss: -12.111186932945593\n",
      "step: 599 - loss: -10.413372291065425\n",
      "step: 600 - loss: -15.17654926910995\n",
      "step: 601 - loss: -13.848051092701326\n",
      "step: 602 - loss: -12.207908007286827\n",
      "step: 603 - loss: -5.387020503500517\n",
      "step: 604 - loss: -13.322051176745695\n",
      "step: 605 - loss: -13.672317617330082\n",
      "step: 606 - loss: -4.604059800409823\n",
      "step: 607 - loss: -10.326449156029545\n",
      "step: 608 - loss: -6.820082487900891\n",
      "step: 609 - loss: -13.098496225340838\n",
      "step: 610 - loss: -13.091184978717994\n",
      "step: 611 - loss: -9.820151586071994\n",
      "step: 612 - loss: -11.449237210905101\n",
      "step: 613 - loss: -4.615767723160502\n",
      "step: 614 - loss: -3.3651453728191782\n",
      "step: 615 - loss: -15.222146439879012\n",
      "step: 616 - loss: -17.664447339044745\n",
      "step: 617 - loss: -11.516262445905834\n",
      "step: 618 - loss: -9.81494211411028\n",
      "step: 619 - loss: -12.481292774553802\n",
      "step: 620 - loss: -8.77545155722214\n",
      "step: 621 - loss: -10.714167604933701\n",
      "step: 622 - loss: -9.542513814564748\n",
      "step: 623 - loss: -16.810081988744592\n",
      "step: 624 - loss: -7.840632428656669\n",
      "step: 625 - loss: -12.74094695825087\n",
      "step: 626 - loss: -12.108171708507268\n",
      "step: 627 - loss: -14.533646707679928\n",
      "step: 628 - loss: -4.330575303876611\n",
      "step: 629 - loss: -9.848009711910594\n",
      "step: 630 - loss: -14.549472719015062\n",
      "step: 631 - loss: -12.036309450815914\n",
      "step: 632 - loss: -13.479528803234668\n",
      "step: 633 - loss: -15.175135696625077\n",
      "step: 634 - loss: -10.633490744446325\n",
      "step: 635 - loss: -15.412999367191413\n",
      "step: 636 - loss: -8.07223974395895\n",
      "step: 637 - loss: -12.885062492430144\n",
      "step: 638 - loss: -13.912717837708737\n",
      "step: 639 - loss: -6.073909091076911\n",
      "step: 640 - loss: -15.980522878038446\n",
      "step: 641 - loss: -14.405550824854327\n",
      "step: 642 - loss: -17.451311223283724\n",
      "step: 643 - loss: -13.657375852495733\n",
      "step: 644 - loss: -17.242959197115987\n",
      "step: 645 - loss: -12.958288675588415\n",
      "step: 646 - loss: -9.911670384286111\n",
      "step: 647 - loss: -7.0098045694557936\n",
      "step: 648 - loss: -12.492070096801898\n",
      "step: 649 - loss: -11.150834258708144\n",
      "step: 650 - loss: -8.893744064898158\n",
      "step: 651 - loss: -13.01480716116525\n",
      "step: 652 - loss: -12.54602420612753\n",
      "step: 653 - loss: -16.710117147928305\n",
      "step: 654 - loss: -13.620922691296627\n",
      "step: 655 - loss: -13.168397373127982\n",
      "step: 656 - loss: -13.594091369581847\n",
      "step: 657 - loss: -13.184342841036415\n",
      "step: 658 - loss: -13.242761483313794\n",
      "step: 659 - loss: -15.821201809100438\n",
      "step: 660 - loss: -7.660274558553257\n",
      "step: 661 - loss: -13.763623923900166\n",
      "step: 662 - loss: -8.872006712605147\n",
      "step: 663 - loss: -10.754827566309135\n",
      "step: 664 - loss: -8.639561680729397\n",
      "step: 665 - loss: -9.07009378920003\n",
      "step: 666 - loss: -11.577162057916563\n",
      "step: 667 - loss: -4.020532955059931\n",
      "step: 668 - loss: -11.358234481000668\n",
      "step: 669 - loss: -15.697257861519947\n",
      "step: 670 - loss: -8.40958914215568\n",
      "step: 671 - loss: -14.283805546419673\n",
      "step: 672 - loss: -14.046074269253195\n",
      "step: 673 - loss: -10.40752591359022\n",
      "step: 674 - loss: -12.740893760321008\n",
      "step: 675 - loss: -8.762426124976596\n",
      "step: 676 - loss: -9.167743747069853\n",
      "step: 677 - loss: -7.038357156310157\n",
      "step: 678 - loss: -12.742346480990316\n",
      "step: 679 - loss: -16.26654874523915\n",
      "step: 680 - loss: -4.574878729172655\n",
      "step: 681 - loss: -6.154383592658888\n",
      "step: 682 - loss: -5.384968569979241\n",
      "step: 683 - loss: -7.789631479782427\n",
      "step: 684 - loss: -7.192726872007537\n",
      "step: 685 - loss: -7.338635033257495\n",
      "step: 686 - loss: -12.773224409319655\n",
      "step: 687 - loss: -5.836056016291675\n",
      "step: 688 - loss: -11.779049307853429\n",
      "step: 689 - loss: -13.333291464460745\n",
      "step: 690 - loss: -8.994286118057087\n",
      "step: 691 - loss: -16.452267886900685\n",
      "step: 692 - loss: -10.398004670426372\n",
      "step: 693 - loss: -10.901785585057926\n",
      "step: 694 - loss: -16.975017312209342\n",
      "step: 695 - loss: -13.922418643225168\n",
      "step: 696 - loss: -15.466242881738845\n",
      "step: 697 - loss: -7.803002239307949\n",
      "step: 698 - loss: -10.433656342277303\n",
      "step: 699 - loss: -11.821978562498565\n",
      "step: 700 - loss: -7.160526501996585\n",
      "step: 701 - loss: -7.730075271553961\n",
      "step: 702 - loss: -12.405928836977186\n",
      "step: 703 - loss: -12.699540334135936\n",
      "step: 704 - loss: -11.928047046116614\n",
      "step: 705 - loss: -11.579146815768652\n",
      "step: 706 - loss: -8.094536198470937\n",
      "step: 707 - loss: -10.260291070809922\n",
      "step: 708 - loss: -9.7542782976664\n",
      "step: 709 - loss: -12.109831952024722\n",
      "step: 710 - loss: -15.558349433703885\n",
      "step: 711 - loss: -11.30545086376737\n",
      "step: 712 - loss: -14.92327695639235\n",
      "step: 713 - loss: -15.029364567473547\n",
      "step: 714 - loss: -16.518623948199675\n",
      "step: 715 - loss: -10.173534252560673\n",
      "step: 716 - loss: -5.453882681949204\n",
      "step: 717 - loss: -13.864505043399593\n",
      "step: 718 - loss: -16.43499530218213\n",
      "step: 719 - loss: -13.015056424759859\n",
      "step: 720 - loss: -12.835366048358336\n",
      "step: 721 - loss: -5.618628085486051\n",
      "step: 722 - loss: -11.87039346550575\n",
      "step: 723 - loss: -5.6293148176161285\n",
      "step: 724 - loss: -11.96060942406222\n",
      "step: 725 - loss: -14.055026199037215\n",
      "step: 726 - loss: -14.535252260923588\n",
      "step: 727 - loss: -11.408360245454187\n",
      "step: 728 - loss: -11.702546892861232\n",
      "step: 729 - loss: -8.99183465675322\n",
      "step: 730 - loss: -10.362040907451252\n",
      "step: 731 - loss: -14.358410371172045\n",
      "step: 732 - loss: -14.074146835618148\n",
      "step: 733 - loss: -10.90783926959367\n",
      "step: 734 - loss: -16.23228348626029\n",
      "step: 735 - loss: -16.68054170996607\n",
      "step: 736 - loss: -14.441272895679983\n",
      "step: 737 - loss: -1.745599311923094\n",
      "step: 738 - loss: -13.273476130332394\n",
      "step: 739 - loss: -14.697801965166676\n",
      "step: 740 - loss: -16.22079011460255\n",
      "step: 741 - loss: -11.179959966815613\n",
      "step: 742 - loss: -19.1619694522241\n",
      "step: 743 - loss: -21.743587308567474\n",
      "step: 744 - loss: -17.58437514459179\n",
      "step: 745 - loss: -12.720830172794063\n",
      "step: 746 - loss: -6.816189758040602\n",
      "step: 747 - loss: -12.662968732172551\n",
      "step: 748 - loss: -10.661413016185584\n",
      "step: 749 - loss: -12.459314287118229\n",
      "step: 750 - loss: -15.230998819208132\n",
      "step: 751 - loss: -16.062016851131133\n",
      "step: 752 - loss: -6.409236754193688\n",
      "step: 753 - loss: -18.935695810095446\n",
      "step: 754 - loss: -15.69798155168013\n",
      "step: 755 - loss: -20.550876060042743\n",
      "step: 756 - loss: -10.659785578664664\n",
      "step: 757 - loss: -18.538196767448834\n",
      "step: 758 - loss: -10.905636394624711\n",
      "step: 759 - loss: -9.119057035616143\n",
      "step: 760 - loss: -6.9560469858187055\n",
      "step: 761 - loss: -8.436121039452933\n",
      "step: 762 - loss: -6.4356151494797205\n",
      "step: 763 - loss: -10.073466902436351\n",
      "step: 764 - loss: -7.436505394886694\n",
      "step: 765 - loss: -14.529600887001997\n",
      "step: 766 - loss: -5.090886896954547\n",
      "step: 767 - loss: -2.3340732178519463\n",
      "step: 768 - loss: -11.804874679438779\n",
      "step: 769 - loss: -7.326427380319648\n",
      "step: 770 - loss: -7.034590311737223\n",
      "step: 771 - loss: -6.293952769202565\n",
      "step: 772 - loss: -10.632924229404026\n",
      "step: 773 - loss: -11.092019456225405\n",
      "step: 774 - loss: -7.577762209270052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 775 - loss: -7.081251203610383\n",
      "step: 776 - loss: -9.230867280274571\n",
      "step: 777 - loss: -14.494563352363405\n",
      "step: 778 - loss: -11.232169176822651\n",
      "step: 779 - loss: -5.3534398248969435\n",
      "step: 780 - loss: -2.775694084833882\n",
      "step: 781 - loss: -3.4391014669595936\n",
      "step: 782 - loss: -10.637391577313071\n",
      "step: 783 - loss: -7.13904277826725\n",
      "step: 784 - loss: -7.775506060710718\n",
      "step: 785 - loss: -19.128030421946793\n",
      "step: 786 - loss: -8.848970081591505\n",
      "step: 787 - loss: -10.095369582877323\n",
      "step: 788 - loss: -10.292926216688114\n",
      "step: 789 - loss: -7.521958964566379\n",
      "step: 790 - loss: -5.255040057982804\n",
      "step: 791 - loss: -12.197760537871483\n",
      "step: 792 - loss: -5.484234306235511\n",
      "step: 793 - loss: -9.466287174871841\n",
      "step: 794 - loss: -18.967255071939825\n",
      "step: 795 - loss: -14.579304589794567\n",
      "step: 796 - loss: -17.39341516812394\n",
      "step: 797 - loss: -7.211859504872763\n",
      "step: 798 - loss: -15.431558877134071\n",
      "step: 799 - loss: -13.700570973044009\n",
      "step: 800 - loss: -7.686057147356033\n",
      "step: 801 - loss: -17.086720451102487\n"
     ]
    }
   ],
   "source": [
    "alpha = 1e2\n",
    "A_hat = alpha * np.eye(n) - A\n",
    "W = np.linalg.svd(np.random.rand(n, n))[0][:, :k] # shape: (m, k)\n",
    "loss_func = lambda W: np.trace(W.T @ A @ W) + 2 * np.trace(W.T @ B)\n",
    "# loss_func = lambda W: np.trace(W.T @ A_hat @ W - 2 * W.T @ B)\n",
    "prev_loss = 0\n",
    "curr_loss = 0\n",
    "step = 1\n",
    "converged = False\n",
    "\n",
    "while not converged:\n",
    "    M = 2 * A_hat @ W + 2 * B\n",
    "    U, S, V = np.linalg.svd(M, full_matrices=False)\n",
    "    W = U @ V.T\n",
    "    curr_loss = loss_func(W)\n",
    "    \n",
    "    # check convergence\n",
    "    if step != 1:\n",
    "        if False:#np.allclose(prev_loss, curr_loss):\n",
    "            converged = True\n",
    "        if step > 800:\n",
    "            converged = True\n",
    "    print(f\"step: {step} - loss: {curr_loss}\")\n",
    "#     print(W)\n",
    "    \n",
    "    prev_loss = curr_loss\n",
    "    step += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
